{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ad44c27b7540dc",
   "metadata": {},
   "source": [
    "# Computational Game Theory Assignment: Hysteretic Q-Learning\n",
    "\n",
    "**Objective:** Explore and implement Hysteretic Q-Learning to evaluate their cooperative capabilities compared to the standard Q-Learning approach in a grid-based, two-agent environment.\n",
    "\n",
    "In cooperative multi-agent reinforcement learning, agents must learn to work together to reach common goals while managing the risk of conflicting or uncoordinated actions. This notebook guides you through the process of implementing the algorithms to observe how different strategies affect learning in a cooperative setting.\n",
    "\n",
    "### Notebook Outline:\n",
    "\n",
    "1. **Environment Setup**: Develop a custom grid-world environment where two agents must cooperate to achieve shared goals.\n",
    "2. **Q-Learning Implementation**: Implement a standard single-agent Q-Learning algorithm to understand the baseline behavior.\n",
    "3. **Hysteretic Q-Learning Implementation**: Implement hysteresis to balance learning between positive and negative rewards, stabilizing updates in a cooperative setting.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9266efc24b755e3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f3ba0bbd18a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:36:01.661773Z",
     "start_time": "2024-10-31T11:36:01.656740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c583ce7c979e1",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "In this assignment, we’ll use a Cooperative Grid World as our testing ground. This grid-world environment simulates a modular, cooperative scenario where two agents must learn to navigate and reach their respective goal positions while avoiding obstacles and penalties. The design allows for flexible configurations of grid size, agent starting positions, goal locations, and obstacle placements, enabling a range of cooperative challenges.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Grid Layout**: Agents operate on a grid where each cell represents a discrete location.\n",
    "- **Cooperation Requirement**: Both agents must learn to reach their goals while coordinating to avoid penalties from conflicting moves.\n",
    "- **Reward Structure**: The environment includes positive rewards for coordination and reaching goals, with penalties for collisions and miscoordination, promoting cooperative strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6b39d43cb4e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T13:09:53.743961Z",
     "start_time": "2024-10-31T13:09:53.726043Z"
    }
   },
   "outputs": [],
   "source": [
    "class CooperativeGridWorld:\n",
    "    \"\"\"\n",
    "    Cooperative GridWorld environment for multi-agent reinforcement learning.\n",
    "\n",
    "    In this environment, multiple agents must navigate a grid with obstacles to reach \n",
    "    their designated goal positions. Agents are penalized for colliding with obstacles\n",
    "    or each other, and receive rewards for successful coordination (reaching their goals\n",
    "    simultaneously). Mis-coordination penalties are given if only some agents reach their goals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 grid_size: tuple = (5, 5), \n",
    "                 agent_start: tuple = ((0, 0), (4, 4)), \n",
    "                 obstacles: tuple = (), \n",
    "                 goals: tuple = ((2, 2), (2, 3)),\n",
    "                 max_steps: int = 50,\n",
    "                 step_reward: int = -0.01,\n",
    "                 collision_penalty: int = -1,\n",
    "                 coordination_reward: int = 10,\n",
    "                 miscoordination_penalty: int = -10,\n",
    "                 end_on_miscoordination: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the CooperativeGridWorld environment.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        grid_size : tuple\n",
    "            Dimensions of the grid.\n",
    "        agent_start : tuple\n",
    "            Initial positions of agents.\n",
    "        obstacles : tuple\n",
    "            Locations of obstacles on the grid.\n",
    "        goals : tuple\n",
    "            Designated goal locations for each agent.\n",
    "        max_steps : int\n",
    "            Maximum allowed steps in an episode.\n",
    "        step_reward : int\n",
    "            Step penalty to discourage unnecessary movement.\n",
    "        collision_penalty : int\n",
    "            Penalty for collisions between agents or with obstacles.\n",
    "        coordination_reward : int\n",
    "            Reward for agents reaching their respective goals simultaneously.\n",
    "        miscoordination_penalty : int\n",
    "            Penalty if only some agents reach their goals.\n",
    "        end_on_miscoordination : bool\n",
    "            Whether to end the episode if miscoordination occurs.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self._initial_agent_positions = agent_start\n",
    "        self.n_agents = len(self._initial_agent_positions)\n",
    "        \n",
    "        assert all(isinstance(pos, tuple) and len(pos) == 2 for pos in agent_start), \"Agent start must be tuples of (x, y) positions\"\n",
    "        assert all(isinstance(obs, tuple) and len(obs) == 2 for obs in obstacles), \"Obstacles must be tuples of (x, y) positions\"\n",
    "        assert all(isinstance(g_pos, tuple) and len(g_pos) == 2 for g_pos in goals), \"Goals must be tuples of (x, y) positions\"\n",
    "        assert len(goals) == self.n_agents\n",
    "        self.agent_goals = goals\n",
    "        self.obstacles = obstacles\n",
    "        self.max_steps = max_steps\n",
    "        self.step_reward = step_reward\n",
    "        self.collision_penalty = collision_penalty\n",
    "        self.coordination_reward = coordination_reward\n",
    "        self.miscoordination_penalty = miscoordination_penalty\n",
    "        self.end_on_miscoordination = end_on_miscoordination\n",
    "        self.agent_positions = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.agent_positions = list(self._initial_agent_positions)\n",
    "        self.steps = 0\n",
    "        return self.agent_positions\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment for all agents.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        actions : list of int\n",
    "            List of actions for each agent. Actions: 0 = up, 1 = down, 2 = left, 3 = right.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (agent_positions, rewards, done) where:\n",
    "            - agent_positions : list of tuples - new positions of agents after the step.\n",
    "            - rewards : list of int - rewards received by each agent.\n",
    "            - done : bool - whether the episode has ended.\n",
    "        \"\"\"\n",
    "        assert len(actions) == self.n_agents, \"Number of actions must match number of agents\"\n",
    "        reward = self.step_reward\n",
    "        self.steps += 1\n",
    "        done = False\n",
    "\n",
    "        # Update each agent’s position\n",
    "        new_positions = []\n",
    "        for i, action in enumerate(actions):\n",
    "            new_position = self._move(self.agent_positions[i], action)\n",
    "            # Check for obstacles\n",
    "            if new_position not in self.obstacles:\n",
    "                new_positions.append(new_position)\n",
    "            else:\n",
    "                new_positions.append(self.agent_positions[i])\n",
    "                reward += self.collision_penalty\n",
    "\n",
    "        # Resolve collisions iteratively until all collisions are handled\n",
    "        while True:\n",
    "            # Identify all colliding positions\n",
    "            position_counts = {}\n",
    "            for pos in new_positions:\n",
    "                position_counts[pos] = position_counts.get(pos, 0) + 1\n",
    "            \n",
    "            colliding_positions = {pos for pos, count in position_counts.items() if count > 1}\n",
    "\n",
    "            if not colliding_positions:\n",
    "                # Exit loop if no collisions\n",
    "                break\n",
    "\n",
    "            # Revert agents involved in collisions to their original positions\n",
    "            for i, new_pos in enumerate(new_positions):\n",
    "                if new_pos in colliding_positions:\n",
    "                    new_positions[i] = self.agent_positions[i]\n",
    "                    reward += self.collision_penalty\n",
    "\n",
    "        # Update agent positions after checking collisions\n",
    "        self.agent_positions = new_positions\n",
    "\n",
    "        # Calculate coordination rewards and check goal conditions\n",
    "        if all(pos == self.agent_goals[i] for i, pos in enumerate(self.agent_positions)):\n",
    "            reward += self.coordination_reward\n",
    "            done = True\n",
    "        elif any(pos == self.agent_goals[i] for i, pos in enumerate(self.agent_positions)):\n",
    "            reward += self.miscoordination_penalty\n",
    "            if self.end_on_miscoordination:\n",
    "                done = True\n",
    "\n",
    "        done |= self.steps >= self.max_steps\n",
    "        \n",
    "        return self.agent_positions, reward, done\n",
    "\n",
    "    def _move(self, position: tuple, action: int) -> tuple:\n",
    "        \"\"\"Moves an agent based on the chosen action within grid boundaries, collision with obstacles and between agents are managed somewhere else.\"\"\"\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the current grid showing agent positions, obstacles, and goals.\"\"\"\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        for pos in self.obstacles:\n",
    "            grid[pos] = -1  # Obstacle\n",
    "        for goal in self.agent_goals:\n",
    "            grid[goal] = 2  # Goal\n",
    "        for idx, pos in enumerate(self.agent_positions):\n",
    "            grid[pos] = idx + 3  # Agents\n",
    "        print(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09708ad2fd1af",
   "metadata": {},
   "source": [
    "## Environment Configurations for Assignment\n",
    "\n",
    "For this assignment, you’ll experiment with three grid-world environments of size 5×5 designed to highlight different aspects of cooperative agent behavior. These environments will help you test how single-agent and multi-agent algorithms adapt to various navigational constraints.\n",
    "\n",
    "### Environment 1: Single-Agent Pathfinding\n",
    "\n",
    "**Description**: This environment contains a single agent that must navigate from a starting point at (0, 0) to a goal at (4, 4). Obstacles are placed in the grid, but a valid path exists that allows the agent to reach its goal.\n",
    "Objective: Implement a basic Q-learning agent to find the optimal path to the goal, avoiding obstacles and minimizing steps taken.\n",
    "\n",
    "### Environment 2: Two-Agent Cooperative Pathfinding with Separate Paths\n",
    "\n",
    "**Description**: This environment has two agents starting from opposite corners: Agent 1 at (0, 0) and Agent 2 at (4, 4). Each agent’s goal is to reach the other’s starting position. Obstacles are arranged to provide at least two distinct paths, allowing both agents to reach their destinations without interference.\n",
    "\n",
    "### Environment 3: Two-Agent Cooperative Pathfinding with a Unique Path\n",
    "\n",
    "**Description**: Similar to Environment 2, this setup features two agents with opposite starting points and goals. However, the obstacle layout here creates a single viable path, meaning the agents must coordinate closely to avoid blocking one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ffdd467f2dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:59:33.799826Z",
     "start_time": "2024-10-31T11:59:33.787826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Environment Configurations\n",
    "\n",
    "# 1. Single-Agent Environment Configuration\n",
    "single_agent_env_conf = {\n",
    "    \"grid_size\": (5, 5),\n",
    "    \"agent_start\": [(0, 0)],\n",
    "    \"goals\": [(4, 4)],\n",
    "    \"obstacles\": [(1, 1), (1, 3), (3, 1), (3, 3)],\n",
    "    \"max_steps\": 20,\n",
    "}\n",
    "\n",
    "print(\"Single-Agent Environment Configuration:\")\n",
    "pprint(single_agent_env_conf)\n",
    "\n",
    "# 2. Two-Agent Environment with Separate Paths\n",
    "two_agent_env_separate_paths_conf = {\n",
    "    \"grid_size\": (5, 5),\n",
    "    \"agent_start\": [(0, 0), (4, 4)],\n",
    "    \"goals\": [(4, 4), (0, 0)],\n",
    "    \"obstacles\": [(1, 1), (1, 2), (2, 1), (3, 3), (2, 3)],\n",
    "    \"max_steps\": 30,\n",
    "}\n",
    "\n",
    "print(\"\\nTwo-Agent Environment with Separate Paths Configuration:\")\n",
    "pprint(two_agent_env_separate_paths_conf)\n",
    "\n",
    "# 3. Two-Agent Environment with a Unique Path\n",
    "two_agent_env_unique_path_conf = {\n",
    "    \"grid_size\": (5, 5),\n",
    "    \"agent_start\": [(0, 0), (4, 4)],\n",
    "    \"goals\": [(4, 4), (0, 0)],\n",
    "    \"obstacles\": [(1, 0), (1, 1), (1, 3), (1, 4), (3, 0), (3, 1), (3, 3), (3, 4), (2,1), (2,3)],\n",
    "    \"max_steps\": 30,\n",
    "}\n",
    "\n",
    "print(\"\\nTwo-Agent Environment with Unique Path Configuration:\")\n",
    "pprint(two_agent_env_unique_path_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd758b475886be",
   "metadata": {},
   "source": "Let's render the environments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f573d1673f274a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:59:37.175191Z",
     "start_time": "2024-10-31T11:59:37.166792Z"
    }
   },
   "outputs": [],
   "source": [
    "single_agent_env = CooperativeGridWorld(**single_agent_env_conf)\n",
    "two_agent_env_separate_paths = CooperativeGridWorld(**two_agent_env_separate_paths_conf)\n",
    "two_agent_env_unique_path = CooperativeGridWorld(**two_agent_env_unique_path_conf)\n",
    "\n",
    "# Render the environments\n",
    "print(\"\\nSingle-Agent Environment:\")\n",
    "single_agent_env.reset()\n",
    "single_agent_env.render()\n",
    "\n",
    "print(\"\\nTwo-Agent Environment with Separate Paths:\")\n",
    "two_agent_env_separate_paths.reset()\n",
    "two_agent_env_separate_paths.render()\n",
    "\n",
    "print(\"\\nTwo-Agent Environment with Unique Path:\")\n",
    "two_agent_env_unique_path.reset()\n",
    "two_agent_env_unique_path.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d0d5b56e72eca",
   "metadata": {},
   "source": [
    "# Starting Code\n",
    "\n",
    "In this section, we provide the foundational code for implementing the Q-learning agent, along with the essential learning function. This code serves as a starting point for your assignments, allowing you to focus on the implementation of Hysteretic Q-Learning. By building upon this code, you will create agents capable of learning in both single-agent and multi-agent environments. Review the provided code carefully, as it includes important functions and structures that will facilitate your development of the Q-learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd88109ae1f3dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:01:46.794738Z",
     "start_time": "2024-10-31T11:01:46.785044Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma: float, n_actions: int, grid_size: tuple, n_agents: int, seed: int):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Learning agent.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        gamma : float\n",
    "            Discount factor for future rewards.\n",
    "        grid_size : tuple\n",
    "            Size of the grid\n",
    "        n_states : int\n",
    "            Number \n",
    "        n_actions : int\n",
    "            Number of actions available to the agent.\n",
    "        seed: int\n",
    "            Seed for the pseudo random number generator.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.grid_size = grid_size\n",
    "        self.q_values = np.zeros(grid_size * n_agents + (n_actions,))\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def greedy(self, agents_pos: tuple) -> int:\n",
    "        # agent pos is an n-agent tuple of positions lets make it a simple tuple\n",
    "        agents_pos: tuple = sum(agents_pos, ())\n",
    "        return np.argmax(self.q_values[agents_pos])\n",
    "    \n",
    "    def act(self, agent_pos: tuple) -> int:\n",
    "        # TODO: need to implement exploration\n",
    "        return self.greedy(agent_pos)\n",
    "    \n",
    "    def update(self, agents_pos: tuple, action: int, reward: float, done: bool, next_agents_pos: tuple):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aedd6ffd400cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:15:23.792555Z",
     "start_time": "2024-10-31T11:15:23.780013Z"
    }
   },
   "outputs": [],
   "source": [
    "def learn(env: CooperativeGridWorld, agents: list[Agent], n_episodes: int = 10000, eval_every: int = 100) -> tuple[dict, list[Agent]]:\n",
    "    \"\"\"\n",
    "    Train the agents using Q-Learning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : CooperativeGridWorld\n",
    "        The environment to train the agents on.\n",
    "    agents : list of Agent\n",
    "        The agents to train.\n",
    "    n_episodes : int\n",
    "        Number of episodes to train the agents.\n",
    "    eval_every: int\n",
    "        Number of episode to do evaluation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - A dictionary with the rewards obtained in each episode, the length in each episode and the eval rewards and eval length\n",
    "        - The trained agents.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(n_episodes)\n",
    "    lengths = np.zeros(n_episodes)\n",
    "    eval_rewards = []\n",
    "    eval_lengths = []\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [agent.act(state) for agent in agents]\n",
    "            next_state, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            for agent, action in zip(agents, actions):\n",
    "                agent.update(state, action, reward, done, next_state)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        rewards[episode] = total_reward\n",
    "        lengths[episode] = step\n",
    "        \n",
    "        if episode % eval_every == 0:\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            step = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                actions = [agent.greedy(state) for agent in agents]\n",
    "                state, reward, done = env.step(actions)\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "            eval_rewards.append(total_reward)\n",
    "            eval_lengths.append(step)\n",
    "        \n",
    "    metrics = {\"reward\": rewards, \"lengths\": lengths, \"eval_rewards\": np.array(eval_rewards), \"eval_lengths\": np.array(eval_lengths)}\n",
    "    return metrics, agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67792640033fdf",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "First, you need to implement the missing components of the Q-learning agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac56aa25c854b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T11:21:50.696508Z",
     "start_time": "2024-10-31T11:21:50.684604Z"
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, epsilon_max: float = 1.0, epsilon_min: float = 0.05, epsilon_decay: float = 0.9999, learning_rate: float = 0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Learning agent.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        epsilon_max : float\n",
    "            Maximum value for epsilon (exploration rate).\n",
    "        epsilon_min : float\n",
    "            Minimum value for epsilon.\n",
    "        epsilon_decay : float\n",
    "            Decay rate for epsilon.\n",
    "        learning_rate : float\n",
    "            Learning rate for updating Q-values.\n",
    "        \"\"\"\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon_max\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def explore(self, agent_pos: tuple) -> int:\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n",
    "    \n",
    "    def act(self, agent_pos: tuple) -> int:\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n",
    "\n",
    "    \n",
    "    def update(self, agents_pos: tuple, action: int, reward: float, done: bool, next_agents_pos: tuple):\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a0002b7a8aeec",
   "metadata": {},
   "source": [
    "Your next task is to evaluate the performance of the Q-learning agent in the single-agent environment and the performance of independent Q-learning in the multi-agent settings. To ensure the robustness of your findings, average the results over multiple random seeds. After collecting the data, conduct a thorough analysis that includes visualizations such as plots to illustrate trends and comparisons. Feel free to experiment with different variations of the environments to gain deeper insights. You are also encouraged to study the impact of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58416cf07fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9e9d92af8ead7",
   "metadata": {},
   "source": [
    "# Hysteretic Q-Learning\n",
    "\n",
    "Hysteretic Q-Learning is a modification of traditional Q-learning that employs two distinct learning rates, $\\alpha$ and $\\beta$, where $\\beta < \\alpha$. The core idea behind using two learning rates is to enhance the learning stability of the agent by differentiating the updates based on whether it would increase or decrease the Q-value. This approach allows the agent to learn faster from successful actions while being more conservative with updates from less successful ones.\n",
    "\n",
    "### Key Concepts\n",
    "**Dual Learning Rates**:\n",
    "- $\\alpha$ (the primary learning rate) is used to update Q-values for positive td-error, encouraging the agent to reinforce successful behaviors.\n",
    "- $\\beta$ (the secondary learning rate) is used for updating Q-values for negative td-error, which helps stabilize learning by preventing drastic changes to the Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e4482561dbb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:00:40.224624Z",
     "start_time": "2024-10-31T12:00:40.213622Z"
    }
   },
   "outputs": [],
   "source": [
    "class HystereticQLearningAgent(Agent):\n",
    "    def __init__(self, epsilon_max: float, epsilon_min: float, epsilon_decay: float, learning_rate_alpha: float, learning_rate_beta: float, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Hysteretic Q-Learning agent.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        epsilon_max : float\n",
    "            Maximum value for epsilon (exploration rate).\n",
    "        epsilon_min : float\n",
    "            Minimum value for epsilon.\n",
    "        epsilon_decay : float\n",
    "            Decay rate for epsilon.\n",
    "        learning_rate_alpha : float\n",
    "            Learning rate for updating Q-values when the update increases the Q-values.\n",
    "        learning_rate_beta : float\n",
    "            Learning rate for updating Q-values when the update decreases the Q-values.\n",
    "        \"\"\"\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate_alpha = learning_rate_alpha\n",
    "        self.learning_rate_beta = learning_rate_beta\n",
    "        self.epsilon = epsilon_max\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def explore(self, agent_pos: tuple) -> int:\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n",
    "\n",
    "    \n",
    "    def act(self, agent_pos: tuple) -> int:\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n",
    "\n",
    "    \n",
    "    def update(self, agents_pos: tuple, action: int, reward: float, done: bool, next_agents_pos: tuple):\n",
    "        raise NotImplementedError(\"You have to implement this.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d101f7fe20fef",
   "metadata": {},
   "source": [
    "Your next task is to evaluate the performance of Hysteretic Q-learning in the multi-agent environments. To ensure the robustness of your findings, average the results over multiple random seeds. After collecting the data, conduct a thorough analysis that includes visualizations such as plots to illustrate trends and comparisons. Feel free to experiment with different variations of the environments to gain deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41453394c49dce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf603a4694818b6",
   "metadata": {},
   "source": [
    "# More Complex Environments\n",
    "\n",
    "In this section, your objective is to define one or two alternative environments that introduce greater challenges for the agents. By constructing these more complex scenarios, you will have the opportunity to benchmark the performance of both Independent Q-Learning and Hysteretic Q-Learning. After conducting experiments in these environments, carefully analyze and discuss the results from both algorithms. Consider aspects such as learning efficiency, coordination, and robustness to environmental changes. This analysis will help reveal how well each algorithm adapts to more challenging cooperative tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530efabe41938bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:02:44.344402Z",
     "start_time": "2024-10-31T12:02:37.303265Z"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294cdc5-c5f6-47a8-8a36-f5568c99210b",
   "metadata": {},
   "source": [
    "# Different exploration\n",
    "\n",
    "This final task consits in analyzing the impact of Boltzman Exploartion on both Independent Q-Learning and Hysteretic Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40687e2d-b903-496f-a1c0-54705a0d4983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BoltzmanQLearningAgent(QLearningAgent):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d969c33-0c8d-4180-a0be-fad86e14ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmanHystereticQLearningAgent(HystereticQLearningAgent):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
